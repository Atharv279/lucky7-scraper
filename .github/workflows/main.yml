name: Lucky7 Scraper (near-24/7)

on:
  schedule:
    - cron: "*/5 * * * *"   # trigger every 5 minutes
  workflow_dispatch: {}

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 55             # allow this run to last ~50-55 minutes
    concurrency:
      group: lucky7-scrape          # queue runs; no overlap
      cancel-in-progress: false
    permissions:
      contents: write

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install Google Chrome
        uses: browser-actions/setup-chrome@v1

      - name: Install Xvfb (virtual display for non-headless Chrome)
        run: sudo apt-get update && sudo apt-get install -y xvfb

      - name: Install Python deps
        shell: bash
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install selenium webdriver-manager beautifulsoup4
          fi

      - name: Run scraper (GUI via Xvfb) ~50 minutes
        env:
          NOH_USER: ${{ secrets.NOH_USER }}     # set in: Settings → Secrets → Actions
          NOH_PASS: ${{ secrets.NOH_PASS }}
          RUN_SECONDS: "3000"                   # ~50 minutes per run
          MAX_ROUNDS: "0"                       # 0 = ignore round cap (time-based)
          CSV_PATH: "lucky7_data.csv"
        run: |
          xvfb-run -a -s "-screen 0 1600x900x24" python scraper.py

      - name: Show CSV tail
        if: always()
        run: |
          ls -lah
          [ -f lucky7_data.csv ] && tail -n 30 lucky7_data.csv || echo "CSV not created yet"

      - name: Commit CSV changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "data: update CSV [skip ci]"
          file_pattern: |
            lucky7_data.csv
            data/*.csv
