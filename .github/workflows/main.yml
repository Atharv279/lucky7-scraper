name: Lucky7 Scraper (every 20 min)

on:
  schedule:
    - cron: "*/20 * * * *"    # run every 20 minutes
  workflow_dispatch: {}        # allow manual runs

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write          # needed to commit CSV back to the repo

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install Google Chrome
        uses: browser-actions/setup-chrome@v1

      - name: Install Xvfb (virtual display for non-headless Chrome)
        run: sudo apt-get update && sudo apt-get install -y xvfb

      - name: Install Python deps
        shell: bash
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install selenium webdriver-manager beautifulsoup4
          fi

      - name: Run scraper (GUI via Xvfb), capture 20 rounds
        env:
          NOH_USER: ${{ secrets.NOH_USER }}   # add these in Settings → Secrets → Actions
          NOH_PASS: ${{ secrets.NOH_PASS }}
          MAX_ROUNDS: "20"
          CSV_PATH: "lucky7_data.csv"
        run: |
          xvfb-run -a -s "-screen 0 1600x900x24" python scraper.py

      - name: Show CSV tail
        if: always()
        run: |
          ls -lah
          [ -f lucky7_data.csv ] && tail -n 30 lucky7_data.csv || echo "CSV not created yet"

      - name: Commit CSV changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "data: update CSV [skip ci]"
          file_pattern: |
            lucky7_data.csv
            data/*.csv
